\documentclass{frontiersSCNS}

\usepackage{url,lineno}
\linenumbers

\graphicspath{{../figures/}}

\def\keyFont{\fontsize{8}{11}\helveticabold }
\def\firstAuthorLast{Bekolay {et~al.}}
\def\Authors{Trevor Bekolay\,$^{1,*}$,
  Terrence C. Stewart\,$^{1}$, and
  Chris Eliasmith\,$^1$}
\def\Address{$^{1}$Centre for Theoretical Neuroscience,
  University of Waterloo,
  Waterloo, ON, Canada}
\def\corrAuthor{Trevor Bekolay}
\def\corrAddress{Centre for Theoretical Neuroscience,
  David R. Cheriton School of Computer Science,
  University of Waterloo,
  200 University Avenue West,
  Waterloo, ON  N2L~3G1, Canada}
\def\corrEmail{tbekolay@uwaterloo.ca}

\begin{document}
\onecolumn
\firstpage{1}

\title[Benchmarking neuromorphic systems with Nengo]{Benchmarking neuromorphic systems with Nengo}
\author[\firstAuthorLast ]{\Authors}
\address{}
\correspondance{}
\extraAuth{}
\topic{Benchmarks and Challenges for Neuromorphic Engineering}

\maketitle

\begin{abstract}

  Abstract

  \tiny
  % 5--8 keywords
  \keyFont{ \section{Keywords:}  }
\end{abstract}

\section{Introduction}

Benchmarking is a notoriously difficult task.
Benchmarks are often created by
the creators of the tools being benchmarked,
resulting in biased comparisons favoring their tool.
Benchmarking can be an inefficient process,
as the tool being benchmarked
may need changes to collect
certain performance metrics.
Even once that effort is undertaken,
benchmarks often are run a few times
and then forgotten,
quickly becoming obsolete.
We propose that the Nengo test suite
can serve as an unbiased, efficient, and future-proof
set of benchmarks for neuromorphic systems.

Nengo is a rigorously tested software package
for building and simulating
large-scale neural models
that can perform cognitively relevant tasks.
It provides a high-level API (frontend)
that can express large-scale models concisely
and in a platform-independent manner.
Several Nengo-compatible simulators (backends)
have been developed that can
run Nengo models on diverse platforms,
including neuromorphic hardware.
Nengo's separation between frontend and backend,
as well as its extensive test suite,
provide standardized comparisons
between different neuromorphic implementations
with respect to functional performance.

The end result is a set of benchmarks that
are written once but run on all backends,
making new benchmarks easy to implement.
Nengo's primary goal is to
build large-scale functional models,
and hence its codebase contains
tests of functional performance;
it is therefore not intentionally
biased in favor of any particular backend.
While there is effort involved in creating
a Nengo backend for a particular neuromorphic system,
the reason to build such a backend
is to leverage the large-scale modelling interface
provided by Nengo;
the ability to collect benchmarks
is automatically provided to any Nengo backend.
Finally, since the test suite exists
to ensure that Nengo continues to function
correctly, and is run frequently using
continuous integration systems,
it will continue to grow and be run frequently,
rather than becoming obsolete in the future.

???expand Other software that do similar things:
many neural simulators exist, and have tests,
but don't have the high-level API
that Nengo has.
On the other hand, some simulators
have high-level APIs
(Topographica?) but a) aren't functional
and b) can't use diverse backends.

??? In the subsequent sections,
we detail the architecture of Nengo's
frontend and backend,
and describe what any backend
is required to implement.
We explain Nengo's testing framework,
including descriptions of the testing fixtures
used to collect and visualize benchmarks.
We then list the metrics that are collected
in this paper,
and the backends on which we collect metrics.
Finally, we show and discuss
the results of collecting those metrics
for each backend.

??? Name backends and give brief intro to them
in intro. Can then refer to them in other sections.

\section{Background}

There are two key features of Nengo 2.0
that enable rapid benchmarking of neuromorphic systems.
The first is a decoupling of model creation and simulation,
resulting in a platform independent frontend interface
to any backend that implements a certain set of requirements.
The second is a ``fuzzy'' test suite ensuring that
Nengo can be used to create models that solve cognitive tasks.
This test suite makes ample use of test fixtures
to enable data collection while testing on arbitrary backends.

\subsection{Nengo architecture}

Nengo has a strict separation between
the frontend and backend.
The frontend exposes a modelling interface
that uses Python to define models concisely.
Backends are responsible for transforming
those frontend objects into code that
can be run on the target platform.
While each backend must be exposed
through Python, this requirement
does not significantly limit
flexibility in the backend.
A backend can be implemented in C and exposed
through Python bindings,
or can be run as a separate process
managed by the Python backend,
with data transmitted to Python through sockets
or other inter-process communication protocols.

\subsubsection{Frontend interface}

Nengo contains five frontend objects
that validate and store symbolic information
about the neural model to be simulated.
All neural models defined with Nengo
are built with these five building blocks,
including Spaun, a model that performs
eight cognitive tasks with 2.5 million neurons
that use visual input to produce motor output
\cite{spaun}.
A backend only needs to be aware of these
five objects to run Nengo models,
and therefore run the benchmarks discussed in this paper.

The primary abstraction in Nengo is the \textit{Ensemble},
which is a group of neurons.
The activity of these neurons
forms a distributed representation of a numerical vector;
for example, a group of 100 neurons might represent
a three-dimensional vector,
and as that vector changes,
the pattern of neural activity
will change accordingly.
Two parameters are mandatory: the number of neurons,
and the dimensionality of the vector being represented.
There are several optional parameters that
affect how the neurons represent the vector space;
encoders map the vector space into currents
to be injected in the neurons,
maximum firing rates can be specified for each neuron,
the type of neuron model can be specified,
and so on.

The \textit{Node} provides a structure for all
non-neural aspects of a model.
\textit{Nodes} can provide input to a system,
collect output values,
interface with physical sensors and motors,
or provide any other computation needed in a mode.
In the case of the benchmarks presented in this paper,
\textit{Nodes} are used to provide input signals
to \textit{Ensembles}.

The \textit{Connection} connects two objects
(e.g., \textit{Ensembles} or \textit{Nodes}) together.
The two objects being connected are the only mandatory parameters;
the synapse model filtering the connection,
a function applied to the vector
communicated across the connection,
and one or more learning rules
can optionally be specified,
along with several other optional parameters.

The \textit{Probe} denotes that a particular
quantity in the simulation should be recorded
at a particular rate.
The object to be probed must be specified;
the attribute of that object,
a sampling rate, and a synapse model for filtering
can optionally be specified.
The \textit{Probe} provides the main mechanism
for data collection during run time.

An important distinction between typical
parameters and those specified in Nengo
is that many are stochastic,
and not necessarily guaranteed
to be supported by every backend.
The vast majority of numeric parameters,
for example,
are most often specified as probability distributions;
an ensemble's maximum firing rate defaults to
a uniform distribution,
but this could be set to a Gaussian distribution
or a discrete set of options and associated
probabilities.
Other parameters,
most notably an ensemble's neuron model,
may not be supported by a particular backend,
which may raise an error.
Even if a particular neuron model is supported,
it may be implemented differently by each backend;
Nengo does its best to approximate
the high-level specification regardless of
the neuron model that is actually used.

Finally, the \textit{Network} is a
container for the other four objects.
The only parameter on the network
is an optional integer seed;
setting this should make
all of the random factors
in the model deterministic.
Aside from managing the random seed,
the network is responsible for maintaining
a network-specific set of default parameters
for the four other objects.
This results in much shorter and less error-prone
model creation scripts.

An example script showing how Nengo can
express a functional network
with ??? neurons and ??? connections
(representing ??? connection weights)
in ??? lines of code can be seen
in Figure~\ref{???}.

\subsubsection{Backend requirements}

The role of the backend is to take
a single network, which contains
ensembles, nodes, connections, and probes,
and construct the underlying objects necessary
to implement the model specified by that network.
That implementation is exposed to Python
through a \textit{Simulator} object,
which has three required methods
and one required attribute.

The first method is \texttt{\_\_init\_\_},
which is a special Python method for initializing objects.
This method must accept a network as its first argument.
It can then accept optional arguments depending on
the capabilities and requirements of the backend;
the reference backend, for example,
accepts \texttt{dt},
which is the length of each timestep.
The purpose of \texttt{\_\_init\_\_} is to
set up the low-level system
that implements the high-level objects
contained in the provided network.
In the reference backend,
this involves sampling from the distributions
in all of the parameters of all objects,
solving for decoding weights and connection weights,
setting up data structures for probed data,
and so on.
When parts of the model are randomly sampled,
the random factors should become deterministic
when a network's seed is set.

The second method is \texttt{run},
which advances the simulation
by the number of seconds
passed in as a required argument.
Whatever low-level structure was created
in \texttt{\_\_init\_\_} advances forward
for that many real or simulated seconds,
and importantly, any probed data is sampled
according to its sample rate.
In the reference backend,
this advances the simulation
by \texttt{time / dt} timesteps,
where \texttt{dt} is fixed at the start of the simulation
and cannot change;
other backends may have variable-length timesteps,
or may not explicitly track timesteps.

The third method is \texttt{trange},
which returns a sequence of times that correspond
to the times at which data was probed.
In the reference backend, this will be
a straightforward sequence of increasing
multiples of \texttt{dt};
other backends may simulate at variable rates.

Finally, the simulator object exposed by each backend
must have an attribute called \texttt{data},
which exposes a dictionary-like interface
to the data being probed over the course
of the simulation.
The data dictionary should,
at a minimum, provide a mapping from
probe instances to the probed data;
in other words, probe instances are keys
and the probed data are values.

These three methods and one attribute
make up the public interface
that Nengo tests expect from backends.
While the existence of the methods
and attribute are necessary
for every Nengo model,
they are not sufficient;
backends are not required
to implement all of the neuron models
or advanced features that
are available in all other backends.
Backends are responsible for informing
the user when model cannot be implemented
on that backend.
The compliance metric (see section ???)
explicitly tracks the details of what
types of models can be implemented
by each backend.
The details of how each backend
implements high-level frontend model
are out of the scope of this paper.

\subsection{Fuzzy testing}

As of version 2.0.1, Nengo's full test suite contains 618 tests.
Many tests are unit tests
that ensure the frontend API operates as we expect.
212 tests construct a simulator instance,
run the simulator, and test the output of that model
(as exposed through \texttt{trange}
and \texttt{data})
and therefore can be considered ``functional'' tests.
For the remainder of this paper,
we will focus only on these 212 functional tests.

Unlike traditional software testing,
there can be significant variability
in many aspects of a Nengo model.
Many model parameters, for example,
can be randomly generated;
other aspects of a model,
such as injected noise,
are necessarily random.
Noise is a fundamental property
of neuromorphic systems.
The accuracy of any large-scale model
is dependent on many factors,
including the number of neurons used
to implement a particular task.
For this reason,
functional tests can only
ensure that the backend implements
the system described by the frontend
well enough.
Each test must determine what
``well enough'' means for that particular network.
While this introduces some subjectivity
to testing,
we believe this is an acceptable consequence
of the breadth of their application.
When adapting these functional tests for benchmarking,
we record the actual accuracy rather than
ensuring that it is within some tolerances.

Nengo's test suite employs the \texttt{pytest}
testing framework \cite{???}.
\texttt{pytest} enables
expressive testing of pluggable components
(such as backends)
through what are called ``test fixtures.''
Fixtures are exposed to the test suite
as arguments that can be provided
to any test function.
\texttt{pytest} inspects the function signature,
and passes an appropriate value
to the function.
This allows for boilerplate code
to be specified once,
and run for any test that might use it.
The example script in Figure~\ref{???}
demonstrates how these fixtures
look in test scripts.

\subsubsection{Test fixtures}

Nengo's test suite defines several test fixtures
to enable fuzzy testing
that collects and visualizes performance metrics
on multiple backends.

\paragraph{\texttt{rng} \& \texttt{seed}}
enable deterministic testing despite
all of the random factors that affect a model.
Test functions that generate random values
(as inputs to a model, for example)
generate those values using \texttt{rng},
which is a random number generator
with an interface identical to
NumPy's random module.
When a test function creates a network,
it passes in \texttt{seed} to make
the random components
of the frontend-to-backend translation deterministic.
Importantly, these random components
are seeded by hashing a string uniquely
identifying each test function,
ensuring that tolerances must be set
wide enough that changing the seed
should only rarely result in test failures.
The hash function is also seeded
with a global integer,
which is changed periodically to ensure
that tests have good tolerances.

\paragraph{\texttt{Simulator}}
allows for tests to be run with multiple backends.
As detailed in section ???,
each backend must expose a simulator class
that accepts a network.
Every test uses this test fixture rather than
an actual class so that
the backend-specific simulator class is used
when the test is run.
Each backend's test suite loads
the tests defined in Nengo,
but replaces the implementation of
this fixture with their own fixture
returning that particular backend's simulator class.

\paragraph{\texttt{plt}, \texttt{logger}, \& \texttt{analytics}}
allow tests to save artifacts from test runs.
These artifacts are how accuracy and speed metrics
are collected, and how the figures
in this paper are generated.
The \texttt{plt} fixture exposes the Matplotlib \cite{???}
\texttt{pyplot} interface to a test function.
These functions can then analyze and plot
the data generated in a simulation
(exposed from a simulator's \texttt{data} attribute)
to visualize the activity in a network.
Figures are saved in a specified or backend-specific directory.
The \texttt{logger} fixture exposes a logging interface
to save arbitrary text to a specified
or backend-specific directory;
these are useful for summary statistics
that can inspected manually.
For large amounts of data that
requires more analysis,
\texttt{analytics} fixture exposes an interface
to save arbitrary data (in the form of NumPy arrays)
to specified or backend-specific directories.

\paragraph{\texttt{analytics\_data}}
enables comparative testing between
two runs of the same test.
The two runs might represent two versions
of the same backend---one before a
speedup and one after---or two
entirely different backends.
The \texttt{analytics\_data} fixture
provides the results from those two
test runs to a single test function,
allowing for comparisons
between the two results saved
with the \texttt{analytics} fixture.
See Figure~\ref{???} for a
concrete example of using the
\texttt{analytics\_data} fixture.

\section{Methods}

Benchmarking involves collecting performance metrics
for two or more comparable systems.
We collect three metrics in this study,
as a proof of concept that the Nengo test suite
can be used to collect meaningful benchmarks
for neural simulators and neuromorphic hardware.

\subsection{Metrics collected}

\paragraph{Compliance}
is the number of tests that a particular backend
passes successfully, relative to the reference
backend, which must pass all tests.
While this metric can give an indication
of how many features a particular backend supports,
it does not take into account how commonly used
a particular feature is;
for this reason, a backend that has relatively
low compliance can still be very useful
in many situations.

\paragraph{Accuracy}
measures how well a particular backend
implements a desired model.
The actual accuracy metric depends
on the desired model,
but in simple cases can be
the root mean squared error (RMSE)
between a desired signal
and the actual signal decoded
from an ensemble.

\paragraph{Speed}
measures the amount of time it takes
for a backend to run a model.
Primarily, we are concerned with
the amount of time taken
in \texttt{run} calls;
however, we also separately measure how long
each backend spends building each model
(i.e., the time spent in \texttt{\_\_init\_\_}),
as models must be built before being run.

\subsection{Test models}

Unlike the compliance metric,
accuracy and speed are only collected
for tests that define an accuracy metric
(i.e., those that use the \texttt{analytics} fixture).
In this study, we focus on four such tests
that address central functioning aspects
of a wide variety of large-scale briain models
(see \cite{NEF or SPA book or both?}).
??? Each test has several parameters
that can be varied
in order to benchmark extreme cases
(e.g., representing many dimensions
with few neurons).
We test each model with parameters
typically used in large-scale models,
but make these parameterized models
available at \url{???github url}
for those who want to explore extreme cases.

\paragraph{1. Communication channel chain.}
In this model,
five ensembles,
comprised of 500 leaky integrate-and-fire (LIF) neurons total,
are connected in series,
with a communication channel
(i.e., the identity function)
computed across each connection.
This model, therefore, attempts to faithfully
communicate an input signal to
the last ensemble in the chain.
The input signal used is a static vector.
This model tests how
robust each backend is to noise introduced
by representing a vector space
in the activity of spiking neurons.

??? include code for this example, as an appendix?

??? Also include appendix with version numbers used?

\paragraph{2. Two-dimensional product.}
In this model,
the scalar product is computed
from a two-dimensional ensemble
comprised of 100 LIF neurons.
Since the product is nonlinear,
we use a space-filling curve
(the Hilbert curve \cite{???})
as the input signal to ensure that
we sample the entire two-dimensional space.
This model tests each backend's
ability to compute a nonlinear function,
albeit a low-dimensional one.
Despite the low dimensionality,
two-dimensional products are frequently used
in large-scale models;
it is the primary operation needed to support
symbol-like processing using neuron-like elements,
for example \cite{spaun or SPA book}.

\paragraph{3. Controlled oscillator.}
In this model,
a three-dimensional ensemble is recurrently connected
such that the first two dimensions continuously
traverse a limit cycle
(i.e., they implement a cyclic attractor),
and the third dimension controls the speed
and direction of the oscillation.
As input, we provide an initial stimulus
to start the oscillation,
and provide a control signal
to cause oscillation at
2Hz, 1Hz, 0Hz, -1Hz, and -2Hz.
In total, the model is comprised of 600 LIF neurons.
This model tests each backend's ability
to stably implement a dynamical system,
which is required for many cognitive functions
like working memory and motor control.

\paragraph{4. Basal Ganglia sequence.}
In this model, brain structures
known collectively as the basal ganglia
are contructed from 4900 LIF neurons,
organized such that they iterate
through a repeating set of actions.
This model has been used to investigate
action selection and learning \cite{stewart, bekolay}
and the switching time between actions
has been closely mapped to human decision making
\cite{stewart 2010}.
This benchmark tests the ability
to construct this model,
and evaluates the time needed
to transition between actions.

\subsection{Backends tested}

We collected the three benchmark metrics
on five Nengo backends.

\paragraph{1. Reference (\texttt{nengo}).}
The reference backend is designed to run quickly
on any general purpose computer
by using NumPy \cite{???}
for fast vectorized computations.
It is included with the \texttt{nengo} frontend
as \texttt{nengo.Simulator}.
When appropriate, other backends will be benchmarked
relative to the reference backend,
which has the most features but does not target
specialized hardware.

\paragraph{2. Distilled (\texttt{nengo\_distilled}).}
The distilled backend is intended as a teaching tool,
and as a template for building new backends.
It can also run on any general purpose computer,
and also uses NumPy for fast vectorized computations,
but does not aim to implement all of the features
implemented by the reference backend,
and omits some optimizations that obfuscate code.
Therefore, the distilled backend is easier to read
and suitable for learning about Nengo,
but is expected to be slower in terms of run time.

\paragraph{3. OpenCL (\texttt{nengo\_ocl}).}
The OpenCL backend uses the Open Computing Language
(OpenCL; \cite{???})
to run Nengo models on many different computing devices,
including graphical processing units (GPUs),
and field-programmable gate arrays (FPGAs).
In contrast to the distilled backend,
it is designed to run Nengo models
as quickly as possible,
using fast general purpose computing devices
like GPUs and any optimizations available,
at the cost of code readability.

\paragraph{4. Brainstorm software (\texttt{nengo\_brainstorm}).}
The Brainstorm backend is a software implementation
of a new neuromorphic chip
based partly on Neurogrid \cite{???}
currently in development
by the Brain in Silicon lab
at Stanford University.
The software backend does not aim for speed;
instead, it attempts to emulate the hardware
in order to test its applicability
to large-scale neural models.
If the emulated hardware can perform well,
then it follows that actual hardware
will also perform well,
but will be much faster.

\paragraph{5. SpiNNaker hardware (\texttt{nengo\_spinnaker}).}
The SpiNNaker backend \cite{Mundy et al., 2015 in press}
targets the eponymous neuromorphic hardware
developed by Furber et al. \cite{10.1109/JPROC.2014.2304638}.
In contrast to the Brainstorm backend,
this backend targets physical neuromorphic hardware,
and therefore is concerned
both with accuracy and speed.

\subsection{Hardware used for running benchmarks}

??? ask Xuan

SpiNN-5 board

\section{Results}

\subsection{Compliance}

Table~\ref{Tab:01} gives compliance results for
all backends except the SpiNNaker backend,
which is not yet set up to run the Nengo test suite.
Of the four currently running unit tests,
the OpenCL backend has the highest compliance
(other than the reference backend).
The distilled backend has lower compliance
than OpenCL, which follows from its stated goal
of being simple but not necessarily feature-rich.
The Brainstorm backend is the least compliant,
but it is still an early experimental test bed;
this is not necessarily indicative of the
final Brainstorm hardware backend being
less compliant than other backends.

\subsection{Accuracy}

Figure~\ref{fig:01} gives accuracy results
for each backend on the chained communication channel model.
The boxplot shows that all five backends can
implement this model accurately,
despite five sources of potential noise.

Figure~\ref{fig:02} gives accuracy results
for the two-dimensional product model.
In this case, while all of the backends
perform well, the SpiNNaker backend
performs significantly worse than the
other four backends.
Its performance, however, is still well within
acceptable ranges for the two-dimensional product model.

Figure~\ref{fig:03} gives accuracy results
for the controlled oscillator model.
The reference and OpenCL backends perform well here,
as do the distilled and Brainstorm backends.
The SpiNNaker backend performs well in most cases,
but some instances of the model
are significantly less accurate
than the majority of instances.

Figure~\ref{fig:04} gives accuracy results
for the basal ganglia sequence model.
Again, all of the backends perform similarly,
except for the SpiNNaker backend.
All other backends have a transition time
around 43 milliseconds, which matches
the results from Stewart et al. 2010 \cite{???}
for simple actions.
The SpiNNaker backend has a transition time
around 55 milliseconds, which is slower
than the time reported by Stewart et al.
for simple actions, but faster than the
time reported for complex actions.

\subsection{Speed}

Figures~\ref{fig:05} and \ref{fig:06} show
the build and run speeds for all backends, respectively.
While build time is not critical to optimize,
it is worth noting that the reference backend
has consistently fast builds.
The OpenCL backend is also usually fast,
but can be slow for certain types of networks
(e.g., the chained communication channel model).
The distilled and Brainstorm backends---which
have a very similar build process---become
slow when dealing with moderately sized models
(e.g., the basal ganglia sequence model,
which contains 4900 neurons).
The SpiNNaker backend incurs an unavoidable cost
in setting up the hardware,
though it builds the largest model faster than
the distilled and Brainstorm backends.

The run time results are of the utmost importance
when evaluating backends for different applications,
especially those that require real-time interaction,
such as robotics. In general, all backends
perform adequately for the three smaller models.
The OpenCL backend is significantly slower for small
models, like the product and controlled oscillator models.
SpiNNaker, on the other hand, is slower than other backends
for these models, but only because its goal
is to run in real-time even if it is possible to run faster.
(??? confirmed: always runs at real time)
The most important result is for the largest model,
the basal ganglia sequence.
In this model, the distilled and Brainstorm backends
performed very poorly
(though neither has speed as a primary goal).
The reference backend also did not perform well,
operating at nearly eight times slower than real-time.
The OpenCL backend performed well, operating at around
four times slower than real-time.
The best performance was seen by the SpiNNaker backend,
however, which operated at around 1.5 times real-time
on this moderately-sized model.

??? For SpiNNaker, place and route is most costly part of
preparing a sim, not necessarily interfacing with hardware

??? Add something about overhead; calling ``run'' still
results in writing data to the board. Actual simulation
occurs in real-time for SpiNNaker; can safely interact
with robotics, neuromorphic hardware, etc

Despite the fast run times for
the OpenCL and SpiNNaker backends,
we also investigated the possibility
that probes were slowing these backend down,
since probes require moving data from
their native platform (GPU or SpiNNaker board)
to the CPU. However, we found that
removing probes from these models
made almost no impact on their run time
(see Figure~\ref{Fig:07}).

\section{Discussion}

The most important finding from these benchmarks
is that specialized hardware (including GPUs)
can result in significantly faster run times
for certain models,
with little to no cost in accuracy.
The conditions under which the OpenCL backend
performs slower than the reference backend
(e.g., the product and controlled oscillator models)
bear closer inspection to determine the source
of the slowdown.

Similarly, we should investigate
the model instances in which the SpiNNaker backend
is less accurate (e.g., in the controlled oscillator model)
or different from other backends
(e.g., in the basal ganglia sequence model).
??? Product difference might be due to different saturation
properties in SpiNNaker ???

Above and beyond the benchmarking results,
however, we believe that the primary contribution
of this study is to provide evidence that Nengo
has a tested, stable, productive frontend
that can target multiple backends,
and therefore provides a platform
for benchmarking neuromorphic backends,
and other neural simulators.
While this is similar to the goal
of other projects, such a PyNN \cite{???},
Nengo is unique in focusing on large-scale simulations,
rather than attempting to support
detailed single-neuron models
(though this capability is still possible in Nengo;
see \cite{???}).

While we cannot claim that Nengo
solves all of complications that arise
in benchmarking,
we believe that it improves upon
the three major issues identified in the introduction.
First, benchmarking neuromorphic hardware
with Nengo is less biased than
hardware-specific benchmarks,
because the purpose of Nengo is
to make functionally interesting large-scale models.
These models are typically built and tested
in the reference backend,
leaving little opportunity to introduce
hardware-specific optimizations
that can bias benchmarks.

Second, there is less wasted effort
in using Nengo for benchmarking,
as the only requirement for using
Nengo is to develop a backend.
Building a Nengo backend for
a piece of neuromorphic hardware
gives access to all of the models
already developed for it,
which is reason enough to develop a backend.
The ability to collect functionally relevant benchmarks
comes ``for free'' once the backend exists.

Third, these benchmarks are more likely to
remain up-to-date because backend developers
themselves do not have to implement or update
these benchmarks;
they will be implemented to test Nengo's
capabilities as a neural simulator,
as an extension of the existing test suite.
Once implemented, backends that implement
the features that are used in a particular benchmark
should not have to modify their own code
to run those benchmarks on their own hardware.

???concluding remarks

\subsection{Data Sharing}

???put results.zip on figshare

???link to Github

\section*{Disclosure/Conflict-of-Interest Statement}

???probably something about ABR here?

\section*{Author Contributions}

???todo

\section*{Acknowledgments}

???todo
(Andrew Mundy for SpiNNaker help, feedback on first draft,
Xuan for GPU server, Eric for help in getting OCL benchmarks running,
James for implementing most of OCL, Jan for product benchmark)

\paragraph{Funding\textcolon} %% Funding

\bibliographystyle{frontiersinSCNS}
\bibliography{paper}

\section*{Figures}

\begin{table}[!ht]
\textbf{\refstepcounter{table}\label{Tab:01} Table \arabic{table}.}{
  Compliance of four backends.}

\processtable{ }
{\begin{tabular}{lcc}\toprule
 Backend & Number of tests passed & Percentage of tests passed\\\midrule
 Reference (\texttt{nengo}) & 212 & 100\% \\
 Distilled (\texttt{nengo\_distilled}) & 129 & 60.8\% \\
 OpenCL (\texttt{nengo\_ocl}) & 187 & 88.2\% \\
 Brainstorm (\texttt{nengo\_brainstorm}) & 107 & 50.4\% \\\botrule
\end{tabular}}{}
\end{table}

% ??? Make color palette grey-scale friendly

% ??? figure out these colors!

% ??? Add A B C etc labels to figures

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig1}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:01} Figure \arabic{figure}.}
       {Chained communication channel model. (Top) One example instance
         of the model run on the reference backend; the decoded output
         of the five chained two-dimensional ensembles during the first
         12 milliseconds of the simulation are shown.
         Each ensemble reaches approximately the correct value of
         $[-0.6, 0.6]$ (indicated with the solid black line)
         but with slight delays depending on their position in the chain.
         (Bottom) The aggregated accuracy results for each backend on the
         chained communication channel model. The observed metric is the root
         mean-squared error (RMSE) between the decoded output of the last
         ensemble in the chain and the correct value (which is randomly
         selected on each trial), during the last 100 milliseconds of the
         500 millisecond simulation. The box shows the median and
         inter-quartile range, while the whiskers extend to 1.5 times the
         inter-quartile range; outliers are not shown.}
\end{figure}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig2}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:02} Figure \arabic{figure}.}
       {Two-dimensional product model. (Top) One example instance of the model
         run on the reference backend; the decoded input represented
         by the two-dimensional ensemble is shown. The input signal is
         designed to fully explore the two-dimensional vector space
         in the range $[-1, 1]$.
         (Middle) The decoded product represented by an ensemble downstream
         of the ensemble representing the input. It closely matches the
         correct product (indicated with the solid black line).
         (Bottom) The aggregated accuracy results for each backend on the
         two-dimensional product model. The measured metric is the root
         mean-squared error (RMSE) between the decoded output of the ensemble
         representing the product and the correct product, during the whole
         range of inputs provided over 5 simulated seconds (after a 500
         millisecond wait period to allow the input ensemble to reach the
         correct initial state). The box shows the median and
         inter-quartile range, while the whiskers extend to 1.5 times the
         inter-quartile range; outliers are not shown.}
\end{figure}

% ??? Remove left plot

% ??? Make middle a stem plot

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=180mm]{fig3}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:03} Figure \arabic{figure}.}
       {Controlled two-dimensional oscillator model. (Left) One example
         instance of the model run on the reference backend; the decoded
         value represented by one of the two dimensions in the oscillatory
         ensemble is shown. The control signal dictating the frequency of
         oscillation changes every two seconds; in this plot each two second
         period is overlaid. Each signal oscillates at approximately the
         correct frequency (see middle for legend).
         (Middle) The power spectrum of each two second period.
         High power is seen only at the frequency we attempt to induce,
         for each of the five test frequencies.
         (Right) The aggregated accuracy results for each backend on the
         two-dimensional product model. The measured metric is the similarity
         between the power spectrum (calculated using the Fast Fourier
         Transform or FFT) of the decoded output of one of the two oscillatory
         dimensions in the ensemble, and the ideal power spectrum,
         computed on a pure sine wave oscillating at the induced frequency.
         Similarity is computed using a normalized dot product across
         all frequencies. The box shows the median and
         inter-quartile range, while the whiskers extend to 1.5 times the
         inter-quartile range; outliers are not shown.}
\end{figure}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig4}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:04} Figure \arabic{figure}.}
       {Basal Ganglia sequence model. (Top) One example instance
         of the model run on the reference backend; the similarity between
         the current representation in working memory and the six possible
         actions is shown (note that in the full benchmark, ten items are
         used). The model quickly progresses from one action to the next,
         cycling back at the end of the six items. The point at which the
         model switches from selecting one action to another action
         (the transition times) are indicated with dashed grey lines.
         (Bottom) The aggregated accuracy results for each backend on the
         basal ganglia sequence model. The observed metric is the mean
         time to transition from one action to the next. The model is run
         for four seconds, allowing for approximately 90 transitions, depending
         on the transition time. The box shows the median and
         inter-quartile range, while the whiskers extend to 1.5 times the
         inter-quartile range; outliers are not shown.}
\end{figure}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig5}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:05} Figure \arabic{figure}.}
       {Build time for each model on each backend. Each bar represents
         the mean build time across 150 instances of each model;
         error bars are 95\% confidence intervals.}
\end{figure}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig6}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:06} Figure \arabic{figure}.}
       {Run time for each model on each backend. Each bar represents
         the mean run time across 150 instances of each model;
         error bars are 95\% confidence intervals.
         Run time is measured relative to real time;
         e.g., a value of two indicates that the model ran twice as slow
         as real time.}
\end{figure}

\end{document}
