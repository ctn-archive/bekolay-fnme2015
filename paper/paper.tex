\documentclass{frontiersSCNS}

\usepackage{url,lineno}
\linenumbers

\def\keyFont{\fontsize{8}{11}\helveticabold }
\def\firstAuthorLast{Bekolay {et~al.}}
\def\Authors{Trevor Bekolay\,$^{1,*}$,
  Terrence C. Stewart\,$^{1}$,
  Eric Hunsberger\,$^1$,
  Andrew Mundy\,$^2$ and
  Chris Eliasmith\,$^1$}
\def\Address{$^{1}$Centre for Theoretical Neuroscience,
  University of Waterloo,
  Waterloo, ON, Canada\\
  $^{2}$ Advanced Processors Technologies Research Group,
  School of Computer Science,
  University of Manchester,
  Manchester, UK}
\def\corrAuthor{Trevor Bekolay}
\def\corrAddress{Centre for Theoretical Neuroscience,
  David R. Cheriton School of Computer Science,
  University of Waterloo,
  200 University Avenue West,
  Waterloo, ON  N2L~3G1, Canada}
\def\corrEmail{tbekolay@uwaterloo.ca}

\begin{document}
\onecolumn
\firstpage{1}

\title[Benchmarking neuromorphic systems with Nengo]{Benchmarking neuromorphic systems with Nengo}
\author[\firstAuthorLast ]{\Authors}
\address{}
\correspondance{}
\extraAuth{}
\topic{Benchmarks and Challenges for Neuromorphic Engineering}

\maketitle

\begin{abstract}

  Abstract

  \tiny
  % 5--8 keywords
  \keyFont{ \section{Keywords:}  }
\end{abstract}

\section{Introduction}

Benchmarking is a notoriously difficult task.
Benchmarks are often created by
the creators of the tools being benchmarked,
resulting in biased comparisons favoring their tool.
Benchmarking can be an inefficient process,
as the tool being benchmarked
may need changes to collect
certain performance metrics.
Even once that effort is undertaken,
benchmarks often are run a few times
and then forgotten,
quickly becoming obsolete.
We propose that the Nengo test suite
can serve as an unbiased, efficient, and future-proof
set of benchmarks for neuromorphic systems.

Nengo is a rigorously tested software package
for building and simulating
large-scale neural models
that can perform cognitively relevant tasks.
It provides a high-level API (frontend)
that can express large-scale models concisely
and in a platform-independent manner.
Several Nengo-compatible simulators (backends)
have been developed that can
run Nengo models on diverse platforms,
including neuromorphic hardware.
Nengo's separation between frontend and backend
and rigorous test suite
provide standardized comparisons
between different neuromorphic implementations
that are geared toward (???) functional performance,
rather than artificial metrics
that may or may not contribute to functional performance.

The end result is a set of benchmarks that
are written once but run on all backends,
making new benchmarks easy to implement.
Nengo's primary goal is to
build large-scale functional models,
and contains tests of functional performance;
it is therefore not intentionally
biased in favor of any particular backend.
While there is effort involved in creating
a Nengo backend for a particular neuromorphic system,
the reason to build such a backend
is to leverage the large-scale modelling interface
provided by Nengo;
the ability to collect benchmarks
is provided essentially for free.
Finally, since the test suite exists
to ensure that Nengo continues to function
correctly, and is run frequently using
continuous integration systems,
it will continue to grow and be run frequently,
rather than becoming obsolete in the future.

Other software that do similar things:
many neural simulators exist, and have tests,
but don't have the high-level API
that Nengo has.
On the other hand, some simulators
have high-level APIs
(Topographica?) but a) aren't functional
and b) can't use diverse backends.

??? In the subsequent sections,
we detail the architecture of Nengo's
frontend and backend,
and describe what each backend
is required to implement.
We explain Nengo's testing framework,
including descriptions of the testing fixtures
used to collect and visualize benchmarks.
We then list the metrics that will be collected,
and the backends on which we will collect metrics.
Finally, we will show and discuss
the results of collecting those metrics
for each backend.

\section{Background}

There are two key features of Nengo 2.0
that enable rapid benchmarking of neuromorphic systems.
The first is a decoupling of model creation and simulation,
resulting in a platform independent frontend interface
to any backend that implements a certain set of requirements.
The second is a ``fuzzy'' test suite ensuring that
Nengo can be used to create models that solve cognitive tasks.
This test suite makes ample use of test fixtures
to enable data collection while testing on arbitrary backends.

\subsection{Nengo architecture}

Nengo has a strict separation between
the frontend and backend.
The frontend exposes a modelling interface
that uses Python and NumPy \cite{???}
to define models concisely.
Backends are responsible for transforming
those frontend objects into code that
can be run on the target platform.
While each backend must be exposed
through Python, this requirement
does not significantly limit
flexibility in the backend.
A backend can be implemented in C and exposed
through Python bindings,
or can be run as a separate process
managed by the Python backend,
with data transmitted to Python through sockets
or other inter-process communication protocols.

\subsubsection{Frontend interface}

Nengo contains five frontend objects
that validate and store symbolic information
about the neural model that will be simulated.
A lot of reasoning can be done at this symbolic level,
which takes significant burdens off of each backend.

The primary abstraction in Nengo is the \textit{Ensemble},
which is a group of neurons (or neuron-like functions)
that collectively represent a time-varying vector of numbers.
Two parameters are mandatory: the number of neurons,
and the dimensionality of the vector being represented.
There are several optional parameters that
affect how the neurons represent the vector space;
encoders map the vector space into currents
to be injected in the neurons,
maximum firing rates can be specified for each neuron,
the type of neuron model can be specified,
and so on.

The \textit{Node} plays a similar role of representing
time-varying vectors of numbers,
but does so directly, without simulating a group of neurons.
As such, the only mandatory parameter
is what vector should be represented;
this can be specified as a constant vector,
or as a function of time.

The \textit{Connection} connects two objects
(e.g., \textit{Ensembles} or \textit{Nodes}) together.
The two objects being connected are the only mandatory parameters;
the synapse model filtering the connection
and a function applied to the vector
communicated across the connection
can optionally be specified.

The \textit{Probe} denotes that a particular
quantity in the simulation should be recorded
at a particular rate.
The object to be probed must be specified;
the attribute of that object,
a sampling rate, and a synapse model for filtering
can optionally be specified.

An important distinction between typical
parameters and those specified in Nengo
is that many are stochastic,
and not necessarily guaranteed
to be supported by every backend.
The vast majority of numeric parameters,
for example,
are most often specified as probability distributions;
an ensemble's maximum firing rate defaults to
a uniform distribution between 200 and 400 Hz,
but this could be set to a Gaussian distribution
or a discrete set of options and associated
probabilities.
Other parameters,
most notably an ensemble's neuron model,
may not be supported by a particular backend,
which may raise an error.
Even if a particular neuron model is supported,
it may be implemented differently by each backend;
Nengo does its best to approximate
the high-level specification regardless of
the neuron model that is actually used.

Finally, the \textit{Network} is a
container for the other four objects.
The only parameter on the network
is an optional integer seed;
setting this should make
all of the random factors
in the model deterministic.
Aside from managing the random seed,
the network is responsible for maintaining
a network-specific set of default parameters
for the four other objects.
This results in much shorter and less error-prone
model creation scripts.

An example script showing how Nengo can
express a functional network
with ??? neurons and ??? connections
(representing ??? connection weights)
in ??? lines of code can be seen
in Figure~\ref{???}.

\subsubsection{Backend requirements}

The role of the backend is to take
a single network, which contains
ensembles, nodes, connections, and probes,
and construct the underlying objects necessary
to implement the model specified by that network.
That implementation is exposed to Python
through a \textit{Simulator} object,
which has three required methods
and one required attribute.

The first method is \texttt{\_\_init\_\_},
which is a special Python method for initializing objects.
This method must accept a network as its first argument.
It can then accept optional arguments depending on
the capabilities and requirements of the backend;
the reference backend, for example,
accepts \texttt{dt},
which is the length of each timestep.
The purpose of \texttt{\_\_init\_\_} is to
set up the low-level system
that implements the high-level objects
contained in the provided network.
When parts of the model must be
randomly sampled, the random factors
should be identical when a network
In the reference backend,
this involves sampling from the distributions
in all of the parameters of all objects,
solving for decoding weights and connection weights,
setting up data structures for probed data,
and so on.

The second method is \texttt{run},
which advances the simulation
by the number of seconds
passed in as a required argument.
Whatever low-level structure was created
in \texttt{\_\_init\_\_} advances forward
for that many real or simulated seconds,
and importantly, any probed data is sampled
according to its sample rate.
In the reference backend,
this advances the simulation
by \texttt{time / dt} timesteps,
where \texttt{dt} is fixed at the start of the simulation
and cannot change;
other backends may have variable-length timesteps,
or may not explicitly track timesteps.

The third method is \texttt{trange},
which returns a sequence of times that correspond
to the times at which data was probed.
In the reference backend, this will be
a straightforward sequence of increasing
multiples of \texttt{dt};
other backends may simulate at variable rates.

Finally, the simulator object exposed by each backend
must have an attribute called \texttt{data},
which exposes a dictionary-like interface
to the data being probed over the course
of the simulation.
The data dictionary should,
at a minimum, provide a mapping from
probe instances to the probed data;
in other words, probe instances are keys
and the probed data are values.

These three methods and one attribute
make up the public interface
that Nengo tests expect from backends;
it is an example
of the adapter pattern \cite{???}. % Maybe lose?
While the existence of the methods
and attribute are necessary
for every Nengo model,
they are not sufficient;
backends are not required
to implement all of the neuron models
or advanced features that
are available in all other backends.
Backends are responsible for informing
the user when model cannot be implemented
on that backend.
The compliance metric (see section ???)
explicitly tracks the details of what
types of models can be implemented
by each backend.
The details of how each backend
implements high-level frontend model
are out of the scope of this paper.

\subsection{Fuzzy testing}

As of version 2.0.1, Nengo's test suite contains 618 tests.
Some test are unit tests though; how many?
In all, ??? can be considered ``functional'' tests,
as they construct a model,
build a simulator object (see previous sec???),
run that object,
and test that the output of that model
(as exposed through \texttt{trange}
and \texttt{data})
matches the desired output,
to a certain tolerance.

Unlike traditional software testing,
there can be significant variability
in many aspects of a Nengo model.
Many model parameters, for example,
can be randomly generated;
other aspects of a model,
such as injected noise,
are necessarily random.
Noise is a fundamental property
of neuromorphic systems.
The accuracy of any large-scale model
is dependent on many factors,
including the number of neurons used
to implement a particular task.
For this reason,
functional tests can only
ensure that the backend implements
the system described by the frontend
well enough.
Each test must determine what
``well enough'' means for that particular network.
While this introduces some subjectivity
to these benchmarks,
it is nearly impossible to create
completely objective benchmarks.
??? more or remove last bit

Nengo's test suite employs the \texttt{pytest}
testing framework \cite{???}.
\texttt{pytest} enables
expressive testing of pluggable components
(such as backends)
through what are called ``test fixtures.''
Fixtures are exposed to the test suite
as arguments that can be provided
to any test function.
\texttt{pytest} inspects the function signature,
and passes an appropriate value
to the function.
This allows for boilerplate code
to be specified once,
and run for any test that might use it.
The example script in Figure~\ref{???}
demonstrates how these fixtures
look in test scripts.

\subsubsection{Test fixtures}

Nengo's test suite defines several test fixtures
to enable fuzzy testing
that collects and visualizes performance metrics
on multiple backends.

\paragraph{\texttt{rng} \& \texttt{seed}}
enable deterministic testing despite
all of the random factors that affect a model.
Test functions that generate random values
(as inputs to a model, for example)
generate those values using \texttt{rng},
which is a random number generator
with an interface identical to
NumPy's random module.
When a test function creates a network,
it passes in \texttt{seed} to make
the random components
of the frontend-to-backend translation deterministic.
Importantly, these random components
are seeded by hashing a string uniquely
identifying each test function,
ensuring that tolerances must be set
wide enough that changing the seed
should only rarely result in test failures.
The hash function is also seeded
with a global integer,
which is changed periodically to ensure
that tests have good tolerances.

\paragraph{\texttt{Simulator}}
allows for tests to be run with multiple backends.
As detailed in section ???,
each backend must expose a simulator class
that accepts a network.
Every test uses this test fixture rather than
an actual class so that
the backend-specific simulator class is used
when the test is run.
Each backend's test suite loads
the tests defined in Nengo,
but replaces the implementation of
this fixture with their own fixture
returning that particular backend's simulator class.

\paragraph{\texttt{plt}, \texttt{logger}, \& \texttt{analytics}}
allow tests to save artifacts from test runs.
These artifacts are how accuracy and speed metrics
are collected, and how the figures
in this paper are generated.
The \texttt{plt} fixture exposes the Matplotlib \cite{???}
\texttt{pyplot} interface to a test function.
These functions can then analyze and plot
the data generated in a simulation
(exposed from a simulator's \texttt{data} attribute)
to visualize the activity in a network.
Figures are saved in a specified or backend-specific directory.
The \texttt{logger} fixture exposes a logging interface
to save arbitrary text to a specified
or backend-specific directory;
these are useful for summary statistics
that can inspected manually.
For large amounts of data that
requires more analysis,
\texttt{analytics} fixture exposes an interface
to save arbitrary data (in the form of NumPy arrays)
to specified or backend-specific directories.

\paragraph{\texttt{analytics\_data}}
enables comparative testing between
two runs of the same test.
The two runs might represent two versions
of the same backend---one before a
speedup and one after---or two
entirely different backends.
The \texttt{analytics\_data} fixture
provides the results from those two
test runs to a single test function,
allowing for comparisons
between the two results saved
with the \texttt{analytics} fixture.
See Figure~\ref{???} for a
concrete example of this.

\section{Methods}

Benchmarking involves collecting performance metrics
for two or more comparable systems.
Currently, the Nengo test suite collects
three types of performance metrics
across five backends.

\subsection{Metrics collected}

\paragraph{Compliance\textcolon}

\paragraph{Accuracy\textcolon}

\paragraph{Speed\textcolon}

\subsection{Backends tested}

\paragraph{Reference\textcolon}

\paragraph{Simplified reference\textcolon}

\paragraph{OpenCL\textcolon}

\paragraph{SpiNNaker hardware\textcolon}

\paragraph{Neurogrid simulation\textcolon}

\section{Results}

\section{Discussion}

\subsection{Data Sharing}

%% Frontiers supports the policy of data sharing, and authors are advised
%% to make freely available any materials and information described in
%% their article, and any data relevant to the article (while not
%% compromising confidentiality in the context of human-subject research)
%% that may be reasonably requested by others for the purpose of academic
%% and non-commercial research. In regards to deposition of data and data
%% sharing through databases, Frontiers urges authors to comply with the
%% current best practices within their discipline.

\section*{Disclosure/Conflict-of-Interest Statement}

%Frontiers follows the recommendations by the International Committee
%of Medical Journal Editors
%(http://www.icmje.org/ethical_4conflicts.html) which require that all
%financial, commercial or other relationships that might be perceived
%by the academic community as representing a potential conflict of
%interest must be disclosed. If no such relationship exists, authors
%will be asked to declare that the research was conducted in the
%absence of any commercial or financial relationships that could be
%construed as a potential conflict of interest. When disclosing the
%potential conflict of interest, the authors need to address the
%following points: • Did you or your institution at any time receive
%payment or services from a third party for any aspect of the
%submitted work?  • Please declare financial relationships with
%entities that could be perceived to influence, or that give the
%appearance of potentially influencing, what you wrote in the
%submitted work.  • Please declare patents and copyrights, whether
%pending, issued, licensed and/or receiving royalties relevant to the
%work.  • Please state other relationships or activities that readers
%could perceive to have influenced, or that give the appearance of
%potentially influencing, what you wrote in the submitted work.

%% The authors declare that the research was conducted in the absence of
%% any commercial or financial relationships that could be construed as a
%% potential conflict of interest.

\section*{Author Contributions}

%When determining authorship the following criteria should be observed:
%• Substantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND
%• Drafting the work or revising it critically for important intellectual content; AND
%• Final approval of the version to be published ; AND
%• Agreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.
%Contributors who meet fewer than all 4 of the above criteria for authorship should not be listed as authors, but they should be acknowledged. (http://www.icmje.org/roles_a.html)

%% The statement about the authors and contributors can be up to several
%% sentences long, describing the tasks of individual authors referred to
%% by their initials and should be included at the end of the manuscript
%% before the References section.

\section*{Acknowledgments}
%% Acknowledgments

\paragraph{Funding\textcolon} %% Funding

\section*{Supplemental Data}
%% Supplementary Material should be uploaded separately on submission, if
%% there are Supplementary Figures, please include the caption in the
%% same file as the figure. LaTeX Supplementary Material templates can be
%% found in the Frontiers LaTeX folder

\bibliographystyle{frontiersinSCNS}
\bibliography{paper}

\section*{Figures}

\end{document}


%% \begin{table}[!t]
%% \textbf{\refstepcounter{table}\label{Tab:01} Table \arabic{table}.}{
%%   Maximum size of the Manuscript }

%% \processtable{ }
%% {\begin{tabular}{lllll}\toprule
%%  & Abstract max. legth (incl. spaces) & Figures or tables & Manuscript max. length & Final PDF length\\\midrule
%%  Clinical Case Study & & & &\\
%%  Clinical Trial & & & &\\
%%  Hypothesis and Theory & & & &\\
%%  Methods & 2000 characters  & 15 & 12000 words & 12 pages\\
%%  Original Research & & & &\\
%%  Review & & & &\\
%%  Technology Report & & & &\\
%%  Focused Review & 2000 characters & 5 & 5000 words & 5 pages\\
%%  CPC &  1250 characters& 6 & 2500 words & 4 pages\\
%%  Perspective & 1250 characters & 2 & 3000 words & 3 pages\\
%%  Mini Review & & & &\\
%%  Classification & 1250 characters & 10 & 2000 words & 12 pages\\
%%  Editorial & none & none & 1000 words & 1 page \\
%%  Book review & & & &\\
%%  Frontiers Commentary & none & 1 & 1000 words & 1 page\\
%%  General Commentary & & & &\\
%%  Field Grand Challenge & & & &\\
%%  Opinion & none & 1 & 2000 words & 2 pages\\
%%  Specialty Grand Challenge& & & &\\\botrule
%% \end{tabular}}{}
%% \end{table}

%% \begin{figure}[h!]
%% \begin{center}
%% \includegraphics[width=10cm]{logo1}
%% \end{center}
%% \textbf{\refstepcounter{figure}\label{fig:01} Figure \arabic{figure}.}
%%        { Enter the caption for your figure here.  Repeat as necessary
%%          for each of your figures }
%% \end{figure}
