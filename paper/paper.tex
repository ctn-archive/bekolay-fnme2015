\documentclass{frontiersSCNS}

\usepackage{url,lineno}
\linenumbers
\usepackage{inconsolata}

\graphicspath{{../figures/}}

\def\keyFont{\fontsize{8}{11}\helveticabold }
\def\firstAuthorLast{Bekolay {et~al.}}
\def\Authors{Trevor Bekolay\,$^{1,*}$,
  Terrence C. Stewart\,$^{1}$, and
  Chris Eliasmith\,$^1$}
\def\Address{$^{1}$Centre for Theoretical Neuroscience,
  University of Waterloo,
  Waterloo, ON, Canada}
\def\corrAuthor{Trevor Bekolay}
\def\corrAddress{Centre for Theoretical Neuroscience,
  David R. Cheriton School of Computer Science,
  University of Waterloo,
  200 University Avenue West,
  Waterloo, ON  N2L~3G1, Canada}
\def\corrEmail{tbekolay@uwaterloo.ca}

\begin{document}
\onecolumn
\firstpage{1}

\title[Benchmarking neuromorphic systems with Nengo]{Benchmarking neuromorphic systems with Nengo}
\author[\firstAuthorLast ]{\Authors}
\address{}
\correspondance{}
\extraAuth{}
\topic{Benchmarks and Challenges for Neuromorphic Engineering}

\maketitle

\begin{abstract}

  Nengo is a software package for designing and simulating
  large-scale neural models.
  Nengo is architected such that the same Nengo model
  can be simulated on any of several
  Nengo backends with few to no modifications.
  Backends translate a model to specific platforms,
  which include GPUs and neuromorphic hardware.
  Nengo also contains a large test suite
  that can be run with any backend
  and focuses primarily on functional performance.
  We propose that Nengo's large test suite
  can be used to benchmark neuromorphic hardware's
  functional performance and simulation speed
  in an efficient, unbiased, and future-proof manner.
  We implement four benchmark models and show that
  Nengo can collect several benchmark metrics
  across five different backends
  that identify situations in which
  some backends perform
  with different accuracy and speed.

  \tiny
  % 5--8 keywords
  \keyFont{ \section{Keywords:} Nengo, benchmarking, neuromorphic hardware }
\end{abstract}

\section{Introduction}

Benchmarking is a notoriously difficult task.
Benchmarks are often created by
the creators of the tools being benchmarked,
resulting in biased comparisons favoring their tool.
Benchmarking can be an inefficient process,
as the tool being benchmarked
may need changes to collect
certain performance metrics.
Even once that effort is undertaken,
benchmarks often are run a few times
and then forgotten,
quickly becoming obsolete.
We propose that the Nengo test suite
can serve as an unbiased, efficient, and future-proof
set of benchmarks for neuromorphic systems.

Nengo is a rigorously tested software package
for building and simulating
large-scale neural models
that can perform cognitively relevant tasks.
It provides a high-level API (frontend)
that can express large-scale models concisely
and in a platform-independent manner.
Several Nengo-compatible simulators (backends)
have been developed that can
run Nengo models on diverse platforms,
including neuromorphic hardware.
Nengo's separation between frontend and backend,
as well as its extensive test suite,
provide standardized comparisons
between different neuromorphic implementations
with respect to functional performance.

The end result is a set of benchmarks that
are written once but run on all backends,
making new benchmarks easy to implement.
Nengo's primary goal is to
build large-scale functional models,
and hence its codebase contains
tests of functional performance;
it is therefore not intentionally
biased in favor of any particular backend.
While there is effort involved in creating
a Nengo backend for a particular neuromorphic system,
the reason to build such a backend
is to leverage the large-scale modelling interface
provided by Nengo;
the ability to collect benchmarks
is automatically provided to any Nengo backend.
Finally, since the test suite exists
to ensure that Nengo continues to function
correctly, and is run frequently using
continuous integration systems,
it will continue to grow and be run frequently,
rather than becoming obsolete in the future.

Several Nengo backends exist or are in active development.
In this paper, we will run benchmarks
on five backends.
The reference, distilled, and Brainstorm backends
run on general purpose computers,
and focus on aspects other than speed.
The OpenCL backend aims to be a fast backend
that still works on general purpose computers,
but can also take advantage of
graphical processing units (GPUs).
The SpiNNaker backend uses
SpiNNaker neuromorphic hardware to
simulate models in real time
for natural interaction with
physical sensors and actuators.

In the subsequent sections,
we detail the architecture of Nengo's
frontend and backend,
and describe what any backend
is required to implement.
We describe Nengo's testing framework,
including explanations of the testing fixtures
used to collect and visualize benchmarks.
We then list the metrics
that are collected in this paper,
and give further details on the backends benchmarked.
Finally, we show and discuss
the results of collecting those metrics
for each backend.

\section{Background}

There are two key features of Nengo
(starting from version 2.0)
that enable rapid benchmarking of neuromorphic systems.
The first is a decoupling of model creation and simulation,
resulting in a platform independent frontend interface
to any backend that implements a certain set of requirements.
The second is a ``fuzzy'' test suite ensuring that
Nengo can be used to create models that solve cognitive tasks.
The test suite makes ample use of test fixtures
to enable data collection while testing on arbitrary backends.

\subsection{Nengo architecture}

Nengo has a strict separation between
frontend and backend.
The frontend exposes a modelling interface
that uses Python to define models concisely.
Backends are responsible for transforming
those frontend objects into code that
can be run on the target platform.
While each backend must be exposed
through Python, this requirement
does not significantly limit
flexibility in the backend.
A backend can be implemented in C and exposed
through Python bindings,
or can be run as a separate process
managed by the Python backend,
with data transmitted to Python through sockets
or other inter-process communication protocols.

\subsubsection{Frontend interface}

Nengo contains five frontend objects
that validate and store symbolic information
about the neural model to be simulated.
All neural models defined with Nengo
are built with these five building blocks,
including Spaun, a model that performs
eight cognitive tasks with 2.5 million neurons
that use visual input to produce motor output
\citep{eliasmith2012}.
A backend only needs to be aware of these
five objects to run Nengo models,
and therefore run the benchmarks discussed in this paper.

The primary abstraction in Nengo is the \textit{Ensemble},
which is a group of neurons.
The activity of these neurons
is generally taken to be
a distributed representation
of a numerical state vector;
for example, a group of 100 neurons might represent
a three-dimensional vector,
and as that vector changes,
the pattern of neural activity
will change accordingly.
Two parameters are mandatory: the number of neurons,
and the dimensionality of the vector being represented.
There are several optional parameters that
affect how the neurons represent the vector space;
encoders map the vector space into currents
to be injected in the neurons,
maximum firing rates can be specified for each neuron,
the type of neuron model can be specified,
and so on.

The \textit{Node} provides a structure for all
non-neural aspects of a model.
\textit{Nodes} can provide input to a system,
collect output values,
interface with physical sensors and motors,
or provide any other computation needed in a mode.
In the case of the benchmarks presented in this paper,
\textit{Nodes} are used to provide input signals
to \textit{Ensembles}.

The \textit{Connection} connects two objects
(e.g., \textit{Ensembles} or \textit{Nodes}) together.
The two objects being connected are the only mandatory parameters;
the synapse model filtering the connection,
a function applied to the vector
communicated across the connection,
and one or more learning rules
can optionally be specified,
along with several other optional parameters.

The \textit{Probe} provides the main mechanism
for data collection during run time
by denoting that a particular
quantity in the simulation should be recorded
at a particular rate.
The object to be probed must be specified;
the attribute of that object,
a sampling rate, and a synapse model for filtering
can optionally be specified.

An important distinction between typical
parameters and those specified in Nengo
is that many are stochastic,
and not necessarily guaranteed
to be supported by every backend.
The vast majority of numeric parameters,
for example,
are most often specified as probability distributions;
an ensemble's maximum firing rate defaults to
a uniform distribution,
but this could be set to a Gaussian distribution
or a discrete set of options and associated
probabilities.
Other parameters,
most notably an ensemble's neuron model,
may not be supported by a particular backend,
which may raise an error.
Even if a particular neuron model is supported,
it may be implemented differently by each backend;
Nengo does its best to approximate
the high-level specification regardless of
the neuron model that is actually used.

Finally, the \textit{Network} is a
container for the other four objects.
The only parameter on the network
is an optional integer seed;
setting this should make
all of the random factors
in the model deterministic.
Aside from managing the random seed,
the network is responsible for maintaining
a network-specific set of default parameters
for the four other objects.
This results in shorter and less error-prone
model creation scripts.

An example script showing how Nengo can
express a functional network
with 500 neurons and 6 connections
(representing 50,000 connection weights
and 100 direct current injection sites)
in under 20 lines of code
can be seen in Supplementary Figure~2.

It is important to note that,
although the Nengo frontend
is designed to make functional networks
with compact code,
any spiking neural network topology
can be implemented in Nengo.
While we will only show examples
of functional connection between ensembles,
it is also possible
to make direct neuron-to-neuron connections;
functional and direct connections
can coexist in the same network,
as is commonly done in situations
requiring direct inhibitory currents.

\subsubsection{Backend requirements}
\label{Sec:Backend}

The role of the backend is to take
a single network, which contains
ensembles, nodes, connections, and probes,
and construct the underlying objects necessary
to implement the model specified by that network.
That implementation is exposed to Python
through a \textit{Simulator} object,
which has three required methods
and one required attribute.

The first method is \texttt{\_\_init\_\_},
which is a special Python method for initializing objects.
This method must accept a network as its first argument.
It can then accept optional arguments depending on
the capabilities and requirements of the backend;
the reference backend, for example,
accepts \texttt{dt},
which is the length of each timestep.
The purpose of \texttt{\_\_init\_\_} is to
set up the low-level system
that implements the high-level objects
contained in the provided network.
In the reference backend,
this involves sampling from the distributions
in all of the parameters of all objects,
solving for decoding weights and connection weights,
setting up data structures for probed data,
and so on.
When parts of the model are randomly sampled,
the random factors should become deterministic
when a network's seed is set.

The second method is \texttt{run},
which advances the simulation
by the number of seconds
passed in as a required argument.
Whatever low-level structure was created
in \texttt{\_\_init\_\_} advances forward
for that many real or simulated seconds,
and importantly, any probed data is sampled
according to its sample rate.
In the reference backend,
this advances the simulation
by \texttt{time / dt} timesteps,
where \texttt{dt} is fixed at the start of the simulation
and cannot change;
other backends may have variable-length timesteps,
or may not explicitly track timesteps.

The third method is \texttt{trange},
which returns a sequence of times that correspond
to the times at which data was probed.
In the reference backend, this will be
a straightforward sequence of increasing
multiples of \texttt{dt};
other backends may simulate at variable rates.

Finally, the simulator object exposed by each backend
must have an attribute called \texttt{data},
which exposes a dictionary-like interface
to the data being probed over the course
of the simulation.
The data dictionary should,
at a minimum, provide a mapping from
probe instances to the probed data;
in other words, probe instances are keys
and the probed data are values.

These three methods and one attribute
make up the public interface
that Nengo tests expect from backends.
While the existence of these methods
and attribute are necessary
for every Nengo model,
backends are not required
to implement all of the neuron models
or advanced features that
are available in all other backends.
Backends are responsible for informing
the user when a model cannot be implemented
on that backend.
The compliance metric (see Section~\ref{Sec:Metrics})
explicitly tracks what
types of models can be implemented
by each backend.
The details of how each backend
implements high-level frontend models
are out of the scope of this paper.

\subsection{Fuzzy testing}

As of version 2.0.1, Nengo's full test suite contains 618 tests.
Many tests are unit tests
that ensure the frontend API operates as expected.
212 tests construct a simulator instance,
run the simulator, and test the output of that model
(as exposed through \texttt{trange}
and \texttt{data})
and therefore can be considered ``functional'' tests.
For the remainder of this paper,
we will focus only on these 212 functional tests.

Unlike traditional software testing,
there can be significant variability
in many aspects of a Nengo model.
Many model parameters, for example,
can be randomly generated;
other aspects of a model,
such as injected noise,
are necessarily random.
Noise is a fundamental property
of neuromorphic systems.
The accuracy of any large-scale model
is dependent on many factors,
including the number of neurons used
to implement a particular task.
For this reason,
functional tests can only
ensure that the backend implements
the system described by the frontend
well enough.
Each test must determine what
``well enough'' means for that particular network.
While this introduces some subjectivity
to testing,
we believe this is an acceptable consequence
of being able to test across multiple backends.
When adapting these functional tests for benchmarking,
we record the actual accuracy rather than
ensuring that it is within some tolerances.

Nengo's test suite employs the \texttt{pytest}
testing framework.\footnote{Available at \url{http://pytest.org/}.}
\texttt{pytest} enables
expressive testing of pluggable components
(such as backends)
through what are called ``test fixtures.''
Fixtures are exposed to the test suite
as arguments that can be provided
to any test function.
\texttt{pytest} inspects the function signature,
and passes an appropriate value
to the function.
This allows for boilerplate code
to be specified once,
and run for any test that might use it.
A demonstration of how these fixtures
are used in test scripts
can be seen in an example script,
included as Supplementary Figure~2.

\subsubsection{Test fixtures}

Nengo's test suite defines several test fixtures
to enable fuzzy testing
that collects and visualizes performance metrics
on multiple backends.

\paragraph{\texttt{rng} \& \texttt{seed}}
enable deterministic testing despite
all of the random factors that affect a model.
Test functions that generate random values
(as inputs to a model, for example)
generate those values using \texttt{rng},
which is a random number generator
with an interface identical to
NumPy's random module.
When a test function creates a network,
it passes in \texttt{seed} to make
the random components
of the frontend-to-backend translation deterministic.
Importantly, these random components
are seeded by hashing a string uniquely
identifying each test function,
ensuring that tolerances must be set
wide enough that changing the seed
should only rarely result in test failures.
The hash function is also seeded
with a global integer,
which is changed periodically to ensure
that tests have good tolerances.

\paragraph{\texttt{Simulator}}
allows for tests to be run with multiple backends.
As detailed in Section~\ref{Sec:Backend},
each backend must expose a simulator class
that accepts a network.
Every test uses this test fixture rather than
an actual class so that
the backend-specific simulator class is used
when the test is run.
Each backend's test suite loads
the tests defined in Nengo,
but replaces the implementation of
this fixture with their own fixture
returning that particular backend's simulator class.

\paragraph{\texttt{plt}, \texttt{logger}, \& \texttt{analytics}}
allow tests to save artifacts from test runs.
These artifacts are how accuracy and speed metrics
are collected, and how the figures
in this paper are generated.
The \texttt{plt} fixture exposes the Matplotlib \citep{hunter2007}
\texttt{pyplot} interface to a test function.
Test functions can then analyze and plot
the data generated in a simulation
(exposed from a simulator's \texttt{data} attribute)
to visualize the activity in a network.
Figures are saved in a specified or backend-specific directory.
The \texttt{logger} fixture exposes a logging interface
to save arbitrary text to a specified
or backend-specific directory;
these are useful for summary statistics
that can inspected manually.
For large amounts of data that
requires more analysis,
\texttt{analytics} fixture exposes an interface
to save arbitrary data (in the form of NumPy arrays)
to specified or backend-specific directories.

\paragraph{\texttt{analytics\_data}}
enables comparative testing between
two runs of the same test.
The two runs might represent two versions
of the same backend---one before a
speedup and one after---or two
entirely different backends.
The \texttt{analytics\_data} fixture
provides the results from those two
test runs to a single test function,
allowing for comparisons
between the two results saved
with the \texttt{analytics} fixture.
See Appendix~\ref{App:01} for a
concrete example of using the
\texttt{analytics\_data} fixture.

\section{Methods}

Benchmarking involves collecting performance metrics
for two or more comparable systems.
We collect three metrics in this study,
as a proof of concept that the Nengo test suite
can be used to collect meaningful benchmarks
for neural simulators and neuromorphic hardware.

\subsection{Metrics collected}
\label{Sec:Metrics}

\paragraph{Compliance}
is the number of tests that a particular backend
passes successfully, relative to the reference
backend, which must pass all tests.
While this metric can give an indication
of how many features a particular backend supports,
it does not take into account how commonly used
a particular feature is;
for this reason, a backend that has relatively
low compliance can still be useful
in many situations.

\paragraph{Accuracy}
measures how well a particular backend
implements a desired model.
The actual accuracy metric depends
on the desired model,
but in simple cases can be
the root mean squared error (RMSE)
between a desired signal
and the actual signal decoded
from an ensemble.

\paragraph{Speed}
measures the amount of time it takes
for a backend to run a model.
Primarily, we are concerned with
the amount of time taken
in \texttt{run} calls;
however, we also separately measure how long
each backend spends building each model
(i.e., the time spent in \texttt{\_\_init\_\_}),
as models must be built before being run.

It should be noted that
during a \texttt{run} call,
each backend incurs some overhead
to manage the simulation in addition
to actually moving the simulation forward.
The amount of overhead depends
on the backend; those on specific hardware platforms
(like the SpiNNaker and OpenCL backends)
are likely to incur more overhead
than those on general purpose platforms.
However, we include the overhead in
our speed benchmarking because
it is not possible to distinguish
overhead from actual run time
in a backend-agnostic way.

\subsection{Test models}

Unlike the compliance metric,
accuracy and speed are only collected
for tests that define an accuracy metric
(i.e., those that use the \texttt{analytics} fixture).
In this study, we focus on four such tests
that address central functioning aspects
of a wide variety of large-scale briain models
(see \citealp{eliasmith2003a,eliasmith2013}).

Each test has several parameters
that can be varied
in order to benchmark a wide variety of cases
(see Appendix~\ref{App:01} for examples).
We test each model with parameters
typically used in large-scale models,
but make these parameterized models
available at \url{https://github.com/ctn-archive/bekolay-fnme2015}
for those who want to explore additional cases.

\paragraph{1. Communication channel chain.}
In this model,
five ensembles,
comprised of 500 leaky integrate-and-fire (LIF) neurons total,
are connected in series,
with a communication channel
(i.e., the identity function)
computed across each connection.
This model, therefore, attempts to faithfully
communicate an input signal to
the last ensemble in the chain.
The input signal used is a static vector.
This model tests how
robust each backend is to noise introduced
by representing a vector space
in the activity of spiking neurons.

\paragraph{2. Two-dimensional product.}
In this model,
the scalar product is computed
from a two-dimensional ensemble
comprised of 100 LIF neurons.
Since the product is nonlinear,
we use a space-filling curve
(the Hilbert curve; \citealp{hilbert1891})
as the input signal to ensure that
we sample the entire two-dimensional space.
This model tests each backend's
ability to compute a nonlinear function,
albeit a low-dimensional one.
Despite the low dimensionality,
two-dimensional products are frequently used
in large-scale models;
it is the primary operation needed to support
symbol-like processing using neuron-like elements,
for example \citep{eliasmith2013}.

\paragraph{3. Controlled oscillator.}
In this model,
a three-dimensional ensemble is recurrently connected
such that the first two dimensions continuously
traverse a limit cycle
(i.e., they implement a cyclic attractor),
and the third dimension controls the speed
and direction of the oscillation.
As input, we provide an initial stimulus
to start the oscillation,
and provide a control signal
to cause oscillation at
2Hz, 1Hz, 0Hz, -1Hz, and -2Hz.
In total, the model is comprised of 600 LIF neurons.
This model tests each backend's ability
to stably implement a dynamical system,
which is required for many cognitive functions
like working memory and motor control.

\paragraph{4. Basal Ganglia sequence.}
In this model, brain structures
known collectively as the basal ganglia
are contructed from 4900 LIF neurons,
organized such that they iterate
through a repeating set of actions.
This model has been used to investigate
action selection and learning \citep{stewart2012a}
and the switching time between actions
has been closely mapped to human decision making
\citep{stewart2010}.
This benchmark tests the ability
to construct this model,
and evaluates the time needed
to transition between actions.

We also test an alternate version
of the basal ganglia sequence model
in which some ``passthrough'' nodes
are pruned from the model.
Passthrough nodes are nodes
that collect signals from several sources,
and pass them to other objects unchanged.
They act as hubs
that group together related signals
to reduce the number of connections
that must be made to
a group of related objects.
Since they do no processing,
they can be difficult to deal with
for backends that are designed to
simulate neurons quickly.
The basal ganglia makes liberal use
of passthrough nodes,
so we also test a version of the model
with most passthrough nodes removed.

\subsection{Backends tested}

We collected the three benchmark metrics
on five Nengo backends.

\paragraph{1. Reference (\texttt{nengo}).}
The reference backend is designed to run quickly
on any general purpose computer
by using NumPy \citep{vanderwalt2011}
for fast vectorized computations.
It is included with the \texttt{nengo} frontend
as \texttt{nengo.Simulator}.
When appropriate, other backends will be benchmarked
relative to the reference backend,
which has the most features but does not target
specialized hardware.

\paragraph{2. Distilled (\texttt{nengo\_distilled}).}
The distilled backend is intended as a teaching tool,
and as a template for building new backends.
It can also run on any general purpose computer,
and also uses NumPy for fast vectorized computations,
but does not aim to implement all of the features
implemented by the reference backend,
and omits some optimizations that obfuscate code.
Therefore, the distilled backend is easier to read
and suitable for learning about Nengo,
but is expected to be slower in terms of run time.

\paragraph{3. OpenCL (\texttt{nengo\_ocl}).}
The OpenCL backend uses the Open Computing Language
(OpenCL; \citealp{stone2010})
to run Nengo models on many different computing devices,
including graphical processing units (GPUs),
and field-programmable gate arrays (FPGAs).
In contrast to the distilled backend,
it is designed to run Nengo models
as quickly as possible,
using fast general purpose computing devices
like GPUs and any optimizations available,
at the cost of code readability.

\paragraph{4. Brainstorm software (\texttt{nengo\_brainstorm}).}
The Brainstorm backend is a software implementation
of a new neuromorphic chip
based partly on Neurogrid \citep{benjamin2014}
currently in development
by the Brain in Silicon lab
at Stanford University\footnote{
  Some details available at \url{http://brainstorm.stanford.edu/projects/}.}.
The software backend does not aim for speed;
instead, it attempts to emulate the hardware
in order to test its applicability
to large-scale neural models.
If the emulated hardware can perform well,
then it follows that actual hardware
will also perform well,
but will be much faster.

\paragraph{5. SpiNNaker hardware (\texttt{nengo\_spinnaker}).}
The SpiNNaker backend \citep{mundyinpress}
targets the eponymous neuromorphic hardware
developed by \citet{furber2014}.
In contrast to the Brainstorm backend,
this backend targets physical neuromorphic hardware,
and therefore is concerned
both with accuracy and speed.

\subsection{Benchmarking environment}

All benchmarks were run on a server
running Debian sid.
The server was configured with
two Intel Xeon E5-2650 CPUs
clocked at 2.00GHz,
four Nvidia Tesla C2075 GPUs,
and 64 GB of RAM.
A single 48-chip SpiNN-5 board
was connected through
a 1 GB ethernet port on
the Supermicro X9DRG-QF motherboard
for the SpiNNaker benchmarks.

The versions of software
used for benchmarking can be found in
Supplementary Table~1.

\section{Results}

\subsection{Compliance}

Table~\ref{Tab:01} gives compliance results for
all backends except the SpiNNaker backend,
which is not yet set up to run the Nengo test suite.
Of the four currently running unit tests,
the OpenCL backend has the highest compliance
(other than the reference backend).
The distilled backend has lower compliance
than OpenCL, which follows from its stated goal
of being simple but not necessarily feature-rich.
The Brainstorm backend is the least compliant,
but it is still an early experimental test bed;
this is not necessarily indicative of the
final Brainstorm hardware backend being
less compliant than other backends.

\subsection{Accuracy}

Figure~\ref{fig:01} gives accuracy results
for each backend on the chained communication channel model.
The boxplot shows that all five backends can
implement this model accurately,
despite five layers of processing that include noise.

Figure~\ref{fig:02} gives accuracy results
for the two-dimensional product model.
In this case, while all of the backends
perform well, the SpiNNaker backend
performs worse than the other four backends.
Its performance, however, is still well within
acceptable ranges for the two-dimensional product model.

Figure~\ref{fig:03} gives accuracy results
for the controlled oscillator model.
The reference and OpenCL backends perform well here,
as do the distilled and Brainstorm backends.
The median accuracy of the SpiNNaker is
the same as the distilled and Brainstorm backends,
but there are a high proportion of model instances
with less accuracy than the median,
as compared to other backends.

Figure~\ref{fig:04} gives accuracy results
for the basal ganglia sequence model.
For the version of model with passthrough nodes,
all of the backends perform similarly,
except for the SpiNNaker backend.
All other backends have a transition time
around 43 milliseconds, which matches
the results from \citet{stewart2010}
for simple actions.
The SpiNNaker backend has a median transition time
around 51 milliseconds, which is slower
than the time reported by Stewart et al.
for simple actions, but faster than the
time reported for complex actions;
its interquartile range
is also significantly larger
than that of other backends.
However, its performance for
the version of the model
with far fewer passthrough nodes
is a closer match to the other backends.
Its median transition time
is around 47 milliseconds,
and its interquartile range
is indistinguishable from the other backends.

\subsection{Speed}

Figures~\ref{fig:05} and \ref{fig:06} show
the build and run speeds for all backends, respectively.
While build time is not critical to optimize,
it is worth noting that the reference backend
has consistently fast builds.
The OpenCL backend is also usually fast,
but can be slow for certain types of networks
(e.g., the chained communication channel model).
The distilled and Brainstorm backends---which
have a very similar build process---become
slow when dealing with moderately sized models
(e.g., the basal ganglia sequence model,
which contains 4900 neurons).
The SpiNNaker backend incurs an unavoidable cost
in setting up the hardware
(determining the placement of computational resources,
and generating routes to connect resources),
though it builds the largest model faster than
the distilled and Brainstorm backends.

The run time results are of the utmost importance
when evaluating backends for different applications,
especially those that require real-time interaction,
such as robotics. In general, all backends
perform adequately for the three smaller models.
The OpenCL backend is significantly slower for small
models, like the product and controlled oscillator models.
SpiNNaker, on the other hand, is slower than other backends
for these models, because its goal
is to run in real-time even if it is possible to run faster.
The most important result is for the largest model,
the basal ganglia sequence.
In this model, the distilled and Brainstorm backends
performed very poorly
(though neither has speed as a primary goal).
The reference backend also did not perform well,
operating at nearly six times slower than real-time.
The OpenCL backend performed well, operating at around
three times slower than real-time.
The best performance was seen by the SpiNNaker backend,
however, which operated at around 1.2 times real-time
(including overhead) on this moderately-sized model.
Since the SpiNNaker board always runs at real-time,
this means that around one-sixth of the time taken
for the \texttt{run} call is overhead.

Other backends also incur overhead,
especially when data is recorded
through probes.
We investigated the possibility
that the overhead involved in probes
dominated the actual run time for some backends.
However, we found that
removing probes from these models
made very little impact on
the total time taken in a \texttt{run} call
(see Supplementary Figure~1).

\section{Discussion}

The most important finding from these benchmarks
is that significant speedups can be gained by
running models on specialized hardware and GPUs,
with little to no cost in accuracy.
The conditions under which the OpenCL backend
performs slower than the reference backend
(e.g., the product and controlled oscillator models)
bear closer inspection to determine the source
of the slowdown.
Similarly, we should investigate
the product model instances
in which the SpiNNaker backend
differs from other backends.

It should be noted that although there are clear
relative speed differences between backends,
these models are very small---each would only
be a small component of a larger model.
Spaun, for example, includes multiple copies
or larger versions of all four of the models
benchmarked here.
Additionally, we run them
for very small amounts of time---the longest
simulation runs for 10 simulated seconds.
Real models run for longer amounts of time
to gather data comparable to those gathered
in neuroscientific experiments.
However, we expect that the results
would be similar
in larger models run for longer amount of time.
The distilled and brainstorms backends should
perform slowly, the OpenCL backend should
run faster than the reference backend,
and SpiNNaker will continue to run at real-time.

Above and beyond the benchmarking results,
however, we believe that the primary contribution
of this study is to provide evidence that Nengo
has a tested, stable, productive frontend
that can target multiple backends,
and therefore provides a platform
for benchmarking neuromorphic backends,
and other neural simulators.
While this is similar to the goal
of other projects, most notably PyNN \citep{davison2008},
Nengo is unique in focusing on
large-scale functional simulations,
rather than attempting to support
detailed single-neuron models
(though this capability is still possible in Nengo).
Topographica \citep{bednar2009} plays a similar role,
in that it can interact with multiple neural simulators
with a high-level API, but its API focuses specifically
on models of topographic maps
and other sensory pathways,
rather than focusing on
a wider variety of functions
and dynamics more generally.

While we cannot claim that Nengo
solves all of the complications that arise
in benchmarking,
we believe that it improves upon
the three major issues identified in the introduction.
First, benchmarking neuromorphic hardware
with Nengo is less biased than
hardware-specific benchmarks,
because the purpose of Nengo is
to make functionally interesting large-scale models.
These models are typically built and tested
in the reference backend,
leaving little opportunity to introduce
hardware-specific optimizations
that can bias benchmarks.

Second, there is less wasted effort
in using Nengo for benchmarking,
as the only requirement for using
Nengo is to develop a backend.
Building a Nengo backend for
a piece of neuromorphic hardware
gives access to all of the models
already developed for it,
which is reason enough to develop a backend.
The ability to collect functionally relevant benchmarks
comes ``for free'' once the backend exists.

Third, these benchmarks are more likely to
remain up-to-date because backend developers
themselves do not have to implement or update
these benchmarks;
they will be implemented to test Nengo's
capabilities as a neural simulator,
as an extension of the existing test suite.
Once implemented, backends that implement
the features that are used in a particular benchmark
should not have to modify their own code
to run those benchmarks on their own hardware.

In summary, we have implemented
four benchmark models using the same
testing framework used for Nengo's test suite,
and gathered benchmark results
for five backends,
including an OpenCL backend that targets GPUs
and a backend that targets
SpiNNaker neuromorphic hardware.
All five backends were able to implement
the models accurately,
with the OpenCL and SpiNNaker backends
simulating the largest model much faster
than the reference backend.
We believe that the these benchmarks
can be easily expanded upon
to develop a suite of benchmarks
that can be run by any neuromorphic hardware
or neural simulator
with an associated Nengo backend.
Running these benchmarks in a common environment
and cleanly visualizing the results
could catalyze the development of
neural systems that implement
large-scale functional models
efficiently and accurately.

\subsection{Data Sharing}

All of the software packages discussed in this paper
are available online,
except for the Brainstorm backend,
which has not yet been publicly released
(see Supplementary Table~1).
All of the benchmarks presented here,
as well as the scripts used to generate
the presented figures,
are available at
\url{https://github.com/ctn-archive/bekolay-fnme2015}.
The results of running the benchmarks
are not included in the repository;
however, they can be downloaded separately at
\url{}
to replicate the figures in this paper.

\section*{Disclosure/Conflict-of-Interest Statement}

The authors of this paper
are three of the seven co-founders
of Applied Brain Research (ABR, Inc.),
which is the sole copyright holder
of Nengo and the distilled backend.
While ABR reserves the right
to commercialize Nengo,
it is freely available
for all non-commercial purposes.
As of the writing of this paper,
ABR has not sold any licenses to Nengo,
but has used it
for other revenue-generating projects.

\section*{Author Contributions}

TB adapted the models for benchmarking,
ran the benchmarks,
wrote the text of the paper,
and prepared all of the figures.
TCS wrote initial versions of the benchmark models
and edited text.
CE oversaw all research activities,
and edited text.

\section*{Acknowledgments}

We thank Andrew Mundy for his help
running the SpiNNaker backend
and providing feedback on drafts of this paper.
We thank Jan Gosmann for the initial version
of the product benchmark models,
and for conceptualization and implementation
of the \texttt{analytics} and \texttt{analytics\_data}
fixtures.
We thank Xuan Choo for administering
the server on which the benchmarks were run,
Eric Hunsberger for help in running
the benchmarks on the OpenCL backend,
and James Bergstra for
implementing most of the OpenCL backend.
We thank the Brainstorm group
at Stanford University for making
their experimental hardware emulation backend
available to us.

\paragraph{Funding\textcolon}
NSERC Graduate Fellowships,
NSERC Discovery (grant 261453),
ONR (N000141310419)
AFOSR (FA8655-13-1-3084),
Canada Research Chairs,
and Canadian Foundation for Innovation
Ontario Innovation Trust.

\bibliographystyle{frontiersinSCNS}
\bibliography{paper}

\section*{Figures}

\begin{table}[!ht]
\textbf{\refstepcounter{table}\label{Tab:01} Table \arabic{table}.}{
  Compliance of four backends.}

\processtable{ }
{\begin{tabular}{lcc}\toprule
 Backend & Number of tests passed & Percentage of tests passed\\\midrule
 Reference (\texttt{nengo}) & 212 & 100\% \\
 OpenCL (\texttt{nengo\_ocl}) & 187 & 88.2\% \\
 Distilled (\texttt{nengo\_distilled}) & 129 & 60.8\% \\
 Brainstorm (\texttt{nengo\_brainstorm}) & 107 & 50.4\% \\\botrule
\end{tabular}}{}
\end{table}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig1}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:01} Figure \arabic{figure}.}
       {Chained communication channel model. (A) One example instance
         of the model run on the reference backend; the decoded output
         of the five chained two-dimensional ensembles during the first
         12 milliseconds of the simulation are shown.
         Each ensemble reaches approximately the correct value
         (indicated with the solid black line)
         but with slight delays depending on their position in the chain.
         (B) The aggregated accuracy results for each backend on the
         chained communication channel model. The observed metric is the root
         mean-squared error (RMSE) between the decoded output of the last
         ensemble in the chain and the correct value (which is randomly
         selected on each trial), during the last 100 milliseconds of the
         500 millisecond simulation. The box shows the median and
         inter-quartile range, while the whiskers extend to 1.5 times the
         inter-quartile range; outliers are not shown.}
\end{figure}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig2}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:02} Figure \arabic{figure}.}
       {Two-dimensional product model. (A) One example instance of the model
         run on the reference backend; the decoded input represented
         by the two-dimensional ensemble is shown. The input signal is
         designed to fully explore the two-dimensional vector space
         in the range $[-1, 1]$.
         (B) The decoded product represented by an ensemble downstream
         of the ensemble representing the input. It closely matches the
         correct product (indicated with the solid black line).
         (C) The aggregated accuracy results for each backend on the
         two-dimensional product model. The measured metric is the root
         mean-squared error (RMSE) between the decoded output of the ensemble
         representing the product and the correct product, during the whole
         range of inputs provided over 5 simulated seconds (after a 500
         millisecond wait period to allow the input ensemble to reach the
         correct initial state). The box shows the median and
         inter-quartile range, while the whiskers extend to 1.5 times the
         inter-quartile range; outliers are not shown.}
\end{figure}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig3}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:03} Figure \arabic{figure}.}
       {Controlled two-dimensional oscillator model. (A) One example
         instance of the model run on the reference backend;
         the power spectrum is shown for each two second period,
         colored to indicate the target frequency given by the control signal.
         High power is seen only at the frequency we attempt to induce,
         for each of the five test frequencies.
         (B) The aggregated accuracy results for each backend on the
         two-dimensional product model. The measured metric is the similarity
         between the power spectrum (calculated using the Fast Fourier
         Transform or FFT) of the decoded output of one of the two oscillatory
         dimensions in the ensemble, and the ideal power spectrum,
         computed on a pure sine wave oscillating at the induced frequency.
         Similarity is computed using a normalized dot product across
         all frequencies. The box shows the median and
         inter-quartile range, while the whiskers extend to 1.5 times the
         inter-quartile range; outliers are not shown.}
\end{figure}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig4}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:04} Figure \arabic{figure}.}
       {Basal Ganglia sequence model. (A) One example instance
         of the model run on the reference backend; the similarity between
         the current representation in working memory and the six possible
         actions is shown (note that in the full benchmark, ten items are
         used). The model quickly progresses from one action to the next,
         cycling back at the end of the six items. The point at which the
         model switches from selecting one action to another action
         (the transition times) are indicated with dashed grey lines.
         (B) The aggregated accuracy results for each backend on the
         basal ganglia sequence model, with all passthrough nodes included.
         The observed metric is the mean
         time to transition from one action to the next. The model is run
         for four seconds, allowing for approximately 90 transitions, depending
         on the transition time. The box shows the median and
         inter-quartile range, while the whiskers extend to 1.5 times the
         inter-quartile range; outliers are not shown.
         (C) The aggregated accuracy results for each backend on the
         basal ganglia sequence model, with most passthrough nodes pruned.
         Lines have the same meaning as in B.}
\end{figure}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig5}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:05} Figure \arabic{figure}.}
       {Build time for each model on each backend. Each bar represents
         the mean build time across 50 instances of each model;
         error bars are bootstrapped 95\% confidence intervals.
         BG Sequence * refers to the basal ganglia sequence model
         in which passthrough nodes have been removed.}
\end{figure}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=85mm]{fig6}
\end{center}
\textbf{\refstepcounter{figure}\label{fig:06} Figure \arabic{figure}.}
       {Run speed for each model on each backend. Each bar represents
         the mean run speed across 50 instances of each model;
         error bars are bootstrapped 95\% confidence intervals.
         Run speed is measured as the simulation time plus overhead,
         relative to real time; e.g., a value of two indicates that
         the calling \texttt{run} for a number of seconds takes
         the model two times that number of real seconds to complete the
         \texttt{run} call.
         BG Sequence * refers to the basal ganglia sequence model
         in which passthrough nodes have been removed.}

\end{figure}

\end{document}
